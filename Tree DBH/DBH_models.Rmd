---
title: "DBH models"
output: html_document
---

# data preparation
```{r}
setwd("C:/Users/YUHONG/Documents/R/DataScienceProject")
# load data
data = read.csv("datasets/all_plots.csv", stringsAsFactors = FALSE)
```
```{r}
summary(data)
sapply(data, typeof)
sapply(data[c(2,3,4,5,8,9,10,11,12,13,14,15)], unique)
```

only extract DBH-related data for DBH modelling
```{r}
DBHdata = data[1:9]
```

check NAs and blank
```{r}
# blank "Large", assume "No" where it's blank
DBHdata[DBHdata$Large == "", "Tree.Number"]
DBHdata[DBHdata$Large == "", "Large"] = "No"
```
```{r}
# blank "Alive.at.year.6"
DBHdata[DBHdata$Alive.at.year.6 == "", 1:7]

# 20 rows with blank "Alive.at.year.6" have valid values for "DBH.year.6", assume they have "Yes" for "Alive.at.year.6"
DBHdata[DBHdata$Alive.at.year.6 == "" & !(is.na(DBHdata$DBH.year.6)), "Alive.at.year.6"] = "Yes"

# the rest 8 rows with blank "Alive.at.year.6" have NA for "DBH.year.6", not sure whether the trees are not alive, or just missing records in year6, so no modification.
```
```{r}
# NA in "DBH.year.6", no modification
DBHdata[is.na(DBHdata$DBH.year.6), 1:7]

# 9 NAs for "DBH.year.6", 8 rows are the rows where "Alive.at.year.6" is blank, and one exception.
```
#------------------------------------------------------------------------------------------
Referring to lower priority hypotheses: 
- The rate of natural mortality will be greater in control plots as compared to T1 and T2.
NOT satisfy: all blank Alive.at.year.6 are in T2:Radial.
#-------------------------------------------------------------------------------------------
```{r}
# NA in "DBH.year.0", no modification
DBHdata[is.na(DBHdata$DBH.year.0), 1:7] # 2 NAs for "DBH.year.0"
```
```{r}
# NA in "Class.year.0" and NA in "Class.year.6", no modification
DBHdata[is.na(DBHdata$Class.year.0) | is.na(DBHdata$Class.year.6), ]

# All NA appears where there's NA for DBH (2 in year0, 9 in year6), but 2 exceptions where there's valid values for DBH:
# both in Plot5(T2: Radial), Tree.Number 148, 149, they have extremely large DBH.
```

add Tree.growth
```{r}
DBHdata$Tree.growth = DBHdata$DBH.year.6 - DBHdata$DBH.year.0
```

remove Tree.growth = NA rows and Tree.growth<=0 rows
```{r}
DBHdata[is.na(DBHdata$Tree.growth) | DBHdata$Tree.growth <= 0, ] # rows to be removed
DBHdata = DBHdata[!(is.na(DBHdata$Tree.growth)) & DBHdata$Tree.growth > 0, ]
```

make Treatment and Large factor
```{r}
DBHdata$Treatment = factor(DBHdata$Treatment)
DBHdata$Large = factor(DBHdata$Large)
```






# plots and distribution of variables
distribution of "Tree.growth"
```{r}
summary(DBHdata$Tree.growth)
var(DBHdata$Tree.growth) # quite large variance, variance = twice mean
hist(DBHdata$Tree.growth, breaks = 50)

# super rough comparison
set.seed(0)
hist(DBHdata$Tree.growth, breaks = 50)
hist(rnorm(400, 1.5), breaks = 20, add = TRUE, col = "light blue")
# very skewed and long heavy tail compared to normal distribution

hist(log(DBHdata$Tree.growth), breaks = 50)
hist(rnorm(400, 0.5), breaks = 40, add = TRUE, col = "light blue")
# taking log seems to make the shape closer to normal, but still different from normal
```

try fitting distribution for "Tree.growth"
```{r}
library(fitdistrplus)
descdist(DBHdata$Tree.growth) # closer to gamma distribution
descdist(log(DBHdata$Tree.growth))
```
```{r}
fitdis1 = fitdist(DBHdata$Tree.growth, "norm")
fitdis2 = fitdist(DBHdata$Tree.growth, "gamma")
summary(fitdis1); summary(fitdis2)

par(mfrow = c(2, 2))
denscomp(list(fitdis1, fitdis2), legendtext = c("Normal","Gamma"))
qqcomp(list(fitdis1, fitdis2), legendtext = c("Normal","Gamma"))
cdfcomp(list(fitdis1, fitdis2), legendtext = c("Normal","Gamma"))
ppcomp(list(fitdis1, fitdis2), legendtext = c("Normal","Gamma"))

# Gamma better describes "Tree.growth" than normal, but the positive tail still doesn't fit
```
```{r}
fitdis3 = fitdist(log(DBHdata$Tree.growth), "norm")
summary(fitdis3)

par(mfrow = c(2, 2))
denscomp(fitdis3, legendtext = "Normal")
qqcomp(fitdis3, legendtext = "Normal")
cdfcomp(fitdis3, legendtext = "Normal")
ppcomp(fitdis3, legendtext = "Normal")

# log(Tree.growth) can roughly be normal
```

distribution of DBH.year.0
```{r}
summary(DBHdata$DBH.year.0)
var(DBHdata$DBH.year.0) # quite large variance, variance = twice mean
hist(DBHdata$DBH.year.0, breaks = 100)

# super rough comparison
set.seed(0)
hist(DBHdata$DBH.year.0, breaks = 100)
hist(rnorm(400, 35, 5), breaks = 40, add = TRUE, col = "light blue")
# DBH.year.0 has too many values within 20-30 to fit a normal shape
```

try fitting distribution for "DBH.year.0"
```{r}
descdist(DBHdata$DBH.year.0)
```
```{r}
fitdis4 = fitdist(DBHdata$DBH.year.0, "norm")
summary(fitdis4)

par(mfrow = c(2, 2))
denscomp(fitdis4, legendtext = "Normal")
qqcomp(fitdis4, legendtext = "Normal")
cdfcomp(fitdis4, legendtext = "Normal")
ppcomp(fitdis4, legendtext = "Normal")

# DBH.yea.0 is not quite like normal distribution, its distribution is also quite different from Tree.growth
# curve shape in the QQplot->maybe some transformation will make it closer to normal
```

scatter plots for Tree.growth~DBH.year.0
```{r}
plot(DBHdata$DBH.year.0, DBHdata$Tree.growth, type = "p", xlab = "DBH.year.0", ylab = "Tree.growth")

plot(DBHdata$DBH.year.0, DBHdata$Tree.growth, type = "p", col = DBHdata$Treatment, xlim = c(20, 70), ylim = c(0, 8), xlab = "DBH.year.0", ylab = "Tree.growth(zoom in)")
legend("topright", legend = c("Control", "T1: Gap", "T2: Radial"), fill = c(1, 2, 3))

plot(DBHdata$DBH.year.0, DBHdata$Tree.growth, type = "p", col = DBHdata$Large, xlim = c(20, 70),ylim = c(0, 8), xlab = "DBH.year.0", ylab = "Tree.growth(zoom in)")
legend("topright", legend = c("not large", "large"), fill = c(1, 2))
```
```{r}
par(mfrow = c(3, 4))
for (i in seq(1:12))
  plot(DBHdata[DBHdata$Plot == i, "DBH.year.0"], DBHdata[DBHdata$Plot == i, "Tree.growth"], type = "p", col = DBHdata[DBHdata$Plot == i, "Treatment"], xlim = c(20, 70), ylim = c(0, 8), xlab = "DBH.year.0", ylab = paste("Plot", i, "Tree.growth"))
```
```{r}
par(mfrow = c(3, 4))
for (i in seq(1:12))
  plot(DBHdata[DBHdata$Plot == i, "DBH.year.0"], log(DBHdata[DBHdata$Plot == i, "Tree.growth"]), type = "p", col = DBHdata[DBHdata$Plot == i, "Treatment"], xlim = c(20, 70), ylim = c(-3, 3), xlab = "DBH.year.0", ylab = paste("Plot", i, "log growth"))
```
```{r}
par(mfrow = c(1, 3))
for (i in c("Control", "T1: Gap", "T2: Radial"))
  plot(DBHdata[DBHdata$Treatment == i, "DBH.year.0"], DBHdata[DBHdata$Treatment == i, "Tree.growth"], type = "p", col = DBHdata$Large, xlim = c(20, 70), ylim = c(0, 8), xlab = "DBH.year.0", ylab = paste(i, "Tree.growth"))
```
```{r}
par(mfrow = c(1, 3))
for (i in c("Control", "T1: Gap", "T2: Radial"))
  plot(DBHdata[DBHdata$Treatment == i, "DBH.year.0"], log(DBHdata[DBHdata$Treatment == i, "Tree.growth"]), type = "p", col = DBHdata$Large, xlim = c(20, 70), ylim = c(-3, 3), xlab = "DBH.year.0", ylab = paste(i, "log growth"))
```
#-------------------------------------------------------------------------------------------
Problem with Tree.growth and Tree.growth~DBH.year.0:
1. relatively large variance, data points are scatterred
2. not very clear linear pattern in some plots(eg: plot10, plot11), so regression might not fit well
3. data points are mixed, lots of overlap between each category
(ie. no matter it's T1 or T2, the corresponding Tree.growth distribution overlaps, red and green points are mixed together)
(ie. within each treatment, large/red and not large/black points are mixed together)

-> tried log(Tree.growth), not very effective in plots or in glm models
-> is there other ways to reduce variance?
-> is it possible to shrink the data points to have a more linear pattern?
-> is there some kind of transformation/transformation to higher dimension to seperate the mixed data points?
-> maybe more thoughts, more questions...
#-------------------------------------------------------------------------------------------






# fit simple glm models
glm gaussian
```{r}
# best model derived from Tree.growth~DBH.year.0*Large*Treatment
glm1.0 = glm(Tree.growth~-1+DBH.year.0+Large+DBH.year.0:Large+Treatment:Large, data = DBHdata)
summary(glm1.0)

tss = sum((DBHdata$Tree.growth-mean(DBHdata$Tree.growth))^2) #TSS
rss = sum((DBHdata$Tree.growth-fitted(glm1.0))^2) #RSS
tss; rss
(tss-rss)/tss #R^2

plot(glm1.0)
plot(DBHdata$Tree.growth, fitted(glm1.0), type = "p", col = DBHdata$Treatment)
abline(0, 1)
```
#-------------------------------------------------------------------------------------------
Referring to the rest 5 hypotheses(2 priority1 + 2 priority2 + 1 lower priority):
- Thinning will increase tree growth in both treatments relative to the control.
- Thinning will have an increased effect on the rate of tree growth in T2 as compared to T1.

- Thinning will increase tree growth in the largest tree class size both treatments relative to the control.
- Thinning will have an increased effect on the rate of tree growth in T2 as compared to T1 in the largest tree class size.

- The greater the tree class size the greater the relative increase in the DBH after thinning across both treatments.

Although the gaussian glm is not a very good model(it has very small Rsquare value, very large residuals), the estimated parameter values still satisfy all the 5 hypotheses.
#-------------------------------------------------------------------------------------------

glm gamma
```{r}
# better than glm1.0, with increased Rsquare value and much smaller and "flat" residuals, but still not a good enough model for Tree.growth
glm2.0 = glm(Tree.growth~Treatment+Large+DBH.year.0+Treatment:DBH.year.0+Large:DBH.year.0, family = "Gamma", data = DBHdata)
summary(glm2.0)

tss = sum((DBHdata$Tree.growth-mean(DBHdata$Tree.growth))^2) #TSS
rss = sum((DBHdata$Tree.growth-fitted(glm2.0))^2) #RSS
tss; rss
(tss-rss)/tss #R^2

plot(glm2.0)
plot(DBHdata$Tree.growth, fitted(glm2.0), type = "p", col = DBHdata$Treatment)
abline(0, 1)
```

```{r}
# view instances marked as outliers in the residual plots and Cook's distance plots in both glm1.0 and glm2.0
DBHdata[c("198","141","393","96","81","156"), ] # 4 outof 6 has large Tree.growth > 8
```






# remove marked outliers and fit glm models
```{r}
DBHdrop = subset(DBHdata, !(Tree.Number == 225 | Tree.Number == 478 | Tree.Number == 496 | Tree.Number == 71 | Tree.Number == 77 | Tree.Number == 184))
```

glm gaussian
```{r}
glm1.1 = glm(Tree.growth~-1+DBH.year.0+Large+Treatment+DBH.year.0:Large+Treatment:Large, data = DBHdrop)
summary(glm1.1)

tss = sum((DBHdrop$Tree.growth-mean(DBHdrop$Tree.growth))^2) #TSS
rss = sum((DBHdrop$Tree.growth-fitted(glm1.1))^2) #RSS
tss; rss
(tss-rss)/tss #R^2

plot(glm1.1)
plot(DBHdrop$Tree.growth, fitted(glm1.1), type = "p", col = DBHdrop$Treatment)
abline(0, 1)
```

glm gamma
```{r}
glm2.1 = glm(Tree.growth~Treatment+Large+DBH.year.0+Treatment:DBH.year.0+Large:DBH.year.0, family = "Gamma", data = DBHdrop)
summary(glm2.1)

tss = sum((DBHdrop$Tree.growth-mean(DBHdrop$Tree.growth))^2) #TSS
rss = sum((DBHdrop$Tree.growth-fitted(glm2.1))^2) #RSS
tss; rss
(tss-rss)/tss #R^2

plot(glm2.1)
plot(DBHdrop$Tree.growth, fitted(glm2.1), type = "p", col = DBHdrop$Treatment)
abline(0, 1)
```






# further remove Tree.growth >=8 and fit glm models
```{r}
DBHdrop[DBHdrop$Tree.growth >= 8, ]
DBHremove = DBHdrop[DBHdrop$Tree.growth < 8, ]
```

glm gaussian
```{r}
glm1.2 = glm(Tree.growth~Treatment+Large+DBH.year.0+Large:DBH.year.0+Treatment:Large, data = DBHremove)
summary(glm1.2)

tss = sum((DBHremove$Tree.growth-mean(DBHremove$Tree.growth))^2) #TSS
rss = sum((DBHremove$Tree.growth-fitted(glm1.2))^2) #RSS
tss; rss
(tss-rss)/tss #R^2

plot(glm1.2)
plot(DBHremove$Tree.growth, fitted(glm1.2), type = "p", col = DBHremove$Treatment)
abline(0, 1)
```

glm gamma
```{r}
glm2.2 = glm(Tree.growth~Treatment+Large+DBH.year.0+Treatment:DBH.year.0+Large:DBH.year.0, family = "Gamma", data = DBHremove)
summary(glm2.2)

tss = sum((DBHremove$Tree.growth-mean(DBHremove$Tree.growth))^2) #TSS
rss = sum((DBHremove$Tree.growth-fitted(glm2.2))^2) #RSS
tss; rss
(tss-rss)/tss #R^2

plot(glm2.2)
plot(DBHremove$Tree.growth, fitted(glm2.2), type = "p", col = DBHremove$Treatment)
abline(0, 1)
```






# fit mixed effect models
mixed effect model using all data
```{r}
library(lme4)
mixed1.0 = lmer(Tree.growth~DBH.year.0+Large+Treatment+DBH.year.0:Large+Treatment:Large+(DBH.year.0+Large+Treatment+DBH.year.0:Large+Treatment:Large|Plot), data = DBHdata)
summary(mixed1.0)
VarCorr(mixed1.0)
y.hat1.0 <- fitted(mixed1.0) # Fitted values
int.hat1.0 <- ranef(mixed1.0)[[1]][[1]] # Predicted intercepts
res.hat1.0 <- residuals(mixed1.0) # Estimated residuals
tss = sum((DBHdata$Tree.growth-mean(DBHdata$Tree.growth))^2) #TSS
rss = sum((DBHdata$Tree.growth-y.hat1.0)^2) #RSS
tss; rss; (tss-rss)/tss #R^2
```
```{r}
qqnorm(int.hat1.0, main="Random Intercepts"); qqline(int.hat1.0)
qqnorm(res.hat1.0, main="Residuals"); qqline(res.hat1.0)
plot(y.hat1.0, res.hat1.0, xlab="Fitted Values", ylab="Residuals")
abline(h=0, lty=2)
plot(DBHdata$Tree.growth, y.hat1.0, col = DBHdata$Treatment)
abline(0, 1)
plot(DBHdata$Tree.growth, y.hat1.0, col = DBHdata$Large)
abline(0, 1)
```

mixed effect model using DBHremove
compare model mixed1.1 with mixed1.0: outliers have very significant impact in model performance
? the best model so far? Rsquared value > 0.5, but not the smallest residuals
```{r}
mixed1.1 = lmer(Tree.growth~DBH.year.0+Large+Treatment+DBH.year.0:Large+Treatment:Large+(DBH.year.0+Large+Treatment+DBH.year.0:Large+Treatment:Large|Plot), data = DBHremove)
summary(mixed1.1)
VarCorr(mixed1.1)
y.hat1.1 <- fitted(mixed1.1) # Fitted values
int.hat1.1 <- ranef(mixed1.1)[[1]][[1]] # Predicted intercepts
res.hat1.1 <- residuals(mixed1.1) # Estimated residuals
tss = sum((DBHremove$Tree.growth-mean(DBHremove$Tree.growth))^2) #TSS
rss = sum((DBHremove$Tree.growth-y.hat1.1)^2) #RSS
tss; rss; (tss-rss)/tss #R^2
```
```{r}
qqnorm(int.hat1.1, main="Random Intercepts"); qqline(int.hat1.1)
qqnorm(res.hat1.1, main="Residuals"); qqline(res.hat1.1)
plot(y.hat1.1, res.hat1.1, xlab="Fitted Values", ylab="Residuals")
abline(h=0, lty=2)
plot(DBHremove$Tree.growth, y.hat1.1, col = DBHremove$Treatment)
abline(0, 1)
plot(DBHremove$Tree.growth, y.hat1.1, col = DBHremove$Large)
abline(0, 1)
```



#------------------------------------------------------------------------------------------
Although some models have slightly different predictor terms, all models can give explanation for the 5 hypotheses(apart from the mortality hypotheses).

But for the one lower priority hypotheses (- The greater the tree class size the greater the relative increase in the DBH after thinning across both treatments.), relative increase is not measured.
Not sure about definition: relative increase ?= Tree.growth/DBH.year.0 ?
#-------------------------------------------------------------------------------------------

